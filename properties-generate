#!/usr/bin/env bash
set -ueo pipefail

# Generate Twitter properties file
cat > twitter-source.properties <<TWITTER_PROPERTIES
name=twitter-source
connector.class=com.eneco.trading.kafka.connect.twitter.TwitterSourceConnector
tasks.max=1
topic=test
twitter.consumerkey=$TWITTER_CONSUMER_KEY
twitter.consumersecret=$TWITTER_CONSUMER_SECRET
twitter.token=$TWITTER_ACCESS_TOKEN
twitter.secret=$TWITTER_ACCESS_TOKEN_SECRET

# set output.format to string to output string key/values, it defaults to structured
output.format=string

language=en
#stream.type=sample
stream.type=filter
track.terms=$TWITTER_TRACK_TERMS
# San Francisco OR New York City
#track.locations=-122.75,36.8,-121.75,37.8,-74,40,-73,41
# bbcbreaking,bbcnews,justinbieber
# track.follow=5402612,612473,27260086
TWITTER_PROPERTIES

# Convert PEM files to JKS files
./setup_certs KAFKA

# Output Confluent properties (which get piped to a properties file by the calling script)
echo producer.security.protocol=SSL

echo producer.ssl.truststore.location=$HOME/.truststore.jks
echo producer.ssl.truststore.password=$TRUSTSTORE_PASSWORD

echo producer.ssl.keystore.location=$HOME/.keystore.jks
echo producer.ssl.keystore.password=$KEYSTORE_PASSWORD
echo producer.ssl.key.password=$KEYSTORE_PASSWORD

echo bootstrap.servers=${KAFKA_URL//kafka+ssl:\/\//}
echo rest.port=$PORT

cat <<PROPERTIES
# copied from Kafka Connect distribution etc/kafka/connect-standalone.properties example file
# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will
# need to configure these based on the format they want their data in when loaded from or stored into Kafka
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter

# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply
# it to
key.converter.schemas.enable=true
value.converter.schemas.enable=true

# The internal converter used for offsets and config data is configurable and must be specified, but most users will
# always want to use the built-in default. Offset and config data is never visible outside of Copcyat in this format.
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false
offset.storage.file.filename=/tmp/connect.offsets

# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000
PROPERTIES
